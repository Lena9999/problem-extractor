{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    \n",
    ")\n",
    "from tqdm import tqdm\n",
    "from text_generation_metrics import TextGenerationMetrics\n",
    "from IPython.display import display\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "# for google colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the model and tokenizer\n",
    "model_name = \"ai-forever/ruT5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"input_text\"], max_length=200, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(\n",
    "        examples[\"target_text\"], max_length=60, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Function for preparing a tokenized dataset\n",
    "\n",
    "\n",
    "def prepare_tokenized_dataset(df, tokenize_func):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    return dataset.map(tokenize_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and processing the dataset\n",
    "\n",
    "path_to_dataset = \"path_to_datset\"\n",
    "dataset = pd.read_csv(path_to_dataset, sep=\",\")\n",
    "dataset = dataset[[\"problems\", \"posts\", \"id\"]]\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset.rename(\n",
    "    columns={'posts': 'input_text', 'problems': 'target_text'})\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    dataset[[\"input_text\", \"target_text\", \"id\"]], test_size=0.3, random_state=42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(valid_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "\n",
    "# Tokenization of datasets\n",
    "train_tokenized = prepare_tokenized_dataset(train_df, tokenize_function)\n",
    "valid_tokenized = prepare_tokenized_dataset(valid_df, tokenize_function)\n",
    "test_tokenized = prepare_tokenized_dataset(test_df, tokenize_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"path_to_result\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=6,\n",
    "    fp16=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=valid_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we will try applying the fine-tuned model to arbitrary text to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/path_to_pre_train_model\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_example = \"Температура, и совсем нет настроения\"\n",
    "input_ids = tokenizer(\n",
    "    first_example, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids, max_length=60)\n",
    "\n",
    "# Decoding the result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's evaluate the performance of the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"/path_to_dataset_for_training.csv\"\n",
    "\n",
    "path_to_dataset = path_to_dataset\n",
    "dataset = pd.read_csv(path_to_dataset, sep=\",\")\n",
    "dataset = dataset[[\"problems\", \"posts\", \"id\"]]\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset.rename(\n",
    "    columns={'posts': 'input_text', 'problems': 'target_text'})\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    dataset[[\"input_text\", \"target_text\", \"id\"]], test_size=0.3, random_state=42)\n",
    "\n",
    "valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To calculate the metrics we will generate the answer using a pre-trained model for the training, test and validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(text, max_length=60):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids, max_length=max_length)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def generate_responses_df(df, source_col, target_col=\"T5_pre_trained_gen_text\", max_length=60):\n",
    "    tqdm.pandas(desc=\"Generating responses\")\n",
    "    df[target_col] = df[source_col].progress_apply(\n",
    "        lambda x: generate_response(x, max_length=max_length))\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_responses_batch(df, source_col, target_col=\"T5_pre_trained_gen_text\", batch_size=16, max_length=60):\n",
    "    generated_responses = []\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Generating responses\", unit=\"batch\"):\n",
    "        batch_texts = df[source_col].iloc[i:i + batch_size].tolist()\n",
    "        input_ids = tokenizer(batch_texts, return_tensors=\"pt\",\n",
    "                              padding=True, truncation=True).input_ids.to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids, max_length=max_length)\n",
    "        batch_generated = [tokenizer.decode(\n",
    "            output, skip_special_tokens=True) for output in outputs]\n",
    "        generated_responses.extend(batch_generated)\n",
    "\n",
    "    df[target_col] = generated_responses\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 1657/1657 [17:44<00:00,  1.56it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The end of test_df generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 1656/1656 [11:52<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The end of valid_df generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 7728/7728 [52:29<00:00,  2.45it/s]  \n"
     ]
    }
   ],
   "source": [
    "test_df = generate_responses_df(df=test_df, source_col=\"input_text\")\n",
    "print(\"The end of test_df generation\")\n",
    "valid_df = generate_responses_df(df=valid_df, source_col=\"input_text\")\n",
    "print(\"The end of valid_df generation\")\n",
    "train_df = generate_responses_df(df=train_df, source_col=\"input_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a comparison of metrics for a model fine-tuned on our specific task, evaluated on training, validation, and test datasets. Additionally, we include metrics for the model before fine-tuning (non_finetuned) to highlight the improvement achieved through fine-tuning.\n",
    "\n",
    "To calculate the metrics, we use the __calculate_all_metrics__ function from the __TextGenerationMetrics__ module. The metrics_calculator function returns the average values for all metrics across the entire DataFrame.\n",
    "\n",
    "The function requires as input a __DataFrame__ that contains the target and generated text. Additionally, you need to specify the parameters:\n",
    "\n",
    "- __target_column__ — the name of the column containing the target text.\n",
    "- __generated_column__ — the name of the column containing the generated text in the DataFrame.\n",
    "\n",
    "#### Overall Performance\n",
    "The fine-tuned model demonstrates:\n",
    "\n",
    "- Good accuracy: Reflected in __BLEU__, __ROUGE__, and __METEOR__ scores.\n",
    "- Strong generalization: Consistent performance across training, validation, and test datasets.\n",
    "- Fluency and diversity: Low __perplexity__ and repetition rate highlight its ability to generate natural and coherent outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU Score (average)</th>\n",
       "      <th>Precision (average)</th>\n",
       "      <th>Recall (average)</th>\n",
       "      <th>F1 Score (average)</th>\n",
       "      <th>ROUGE-2 (average)</th>\n",
       "      <th>ROUGE-L (average)</th>\n",
       "      <th>METEOR (average)</th>\n",
       "      <th>Perplexity (average)</th>\n",
       "      <th>Repetition Rate (average)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.348871</td>\n",
       "      <td>0.590895</td>\n",
       "      <td>0.593959</td>\n",
       "      <td>0.573655</td>\n",
       "      <td>0.456144</td>\n",
       "      <td>0.568462</td>\n",
       "      <td>0.548376</td>\n",
       "      <td>8.933747</td>\n",
       "      <td>0.032956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <td>0.302542</td>\n",
       "      <td>0.554859</td>\n",
       "      <td>0.565603</td>\n",
       "      <td>0.540236</td>\n",
       "      <td>0.413150</td>\n",
       "      <td>0.535876</td>\n",
       "      <td>0.519141</td>\n",
       "      <td>8.971618</td>\n",
       "      <td>0.031747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.318592</td>\n",
       "      <td>0.563575</td>\n",
       "      <td>0.562599</td>\n",
       "      <td>0.544328</td>\n",
       "      <td>0.426454</td>\n",
       "      <td>0.537988</td>\n",
       "      <td>0.522028</td>\n",
       "      <td>9.041642</td>\n",
       "      <td>0.036448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non_finetuned</th>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.039536</td>\n",
       "      <td>0.039583</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.030556</td>\n",
       "      <td>0.018473</td>\n",
       "      <td>10.668960</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BLEU Score (average)  Precision (average)  Recall (average)  \\\n",
       "Dataset                                                                      \n",
       "train                      0.348871             0.590895          0.593959   \n",
       "valid                      0.302542             0.554859          0.565603   \n",
       "test                       0.318592             0.563575          0.562599   \n",
       "non_finetuned              0.003709             0.039536          0.039583   \n",
       "\n",
       "               F1 Score (average)  ROUGE-2 (average)  ROUGE-L (average)  \\\n",
       "Dataset                                                                   \n",
       "train                    0.573655           0.456144           0.568462   \n",
       "valid                    0.540236           0.413150           0.535876   \n",
       "test                     0.544328           0.426454           0.537988   \n",
       "non_finetuned            0.030309           0.000575           0.030556   \n",
       "\n",
       "               METEOR (average)  Perplexity (average)  \\\n",
       "Dataset                                                 \n",
       "train                  0.548376              8.933747   \n",
       "valid                  0.519141              8.971618   \n",
       "test                   0.522028              9.041642   \n",
       "non_finetuned          0.018473             10.668960   \n",
       "\n",
       "               Repetition Rate (average)  \n",
       "Dataset                                   \n",
       "train                           0.032956  \n",
       "valid                           0.031747  \n",
       "test                            0.036448  \n",
       "non_finetuned                   0.000908  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_calculator = TextGenerationMetrics()\n",
    "path_to_T5_1_response = pd.read_csv(\n",
    "    \"/path_to_non_finetuned_T5_model_result\", sep=\",\")\n",
    "\n",
    "non_finetuned = metrics_calculator.calculate_all_metrics(\n",
    "    path_to_T5_1_response, target_column='target_text', generated_column='generated_text')\n",
    "train_df_metrics = metrics_calculator.calculate_all_metrics(\n",
    "    train_df, target_column='target_text', generated_column='T5_pre_trained_gen_text')\n",
    "valid_df_metrics = metrics_calculator.calculate_all_metrics(\n",
    "    valid_df, target_column='target_text', generated_column='T5_pre_trained_gen_text')\n",
    "test_df_metrics = metrics_calculator.calculate_all_metrics(\n",
    "    test_df, target_column='target_text', generated_column='T5_pre_trained_gen_text')\n",
    "\n",
    "summary_table = pd.DataFrame(\n",
    "    [train_df_metrics, valid_df_metrics, test_df_metrics, non_finetuned],\n",
    "    index=[\"train\", \"valid\", \"test\", \"non_finetuned\"]\n",
    ")\n",
    "summary_table.index.name = 'Dataset'\n",
    "\n",
    "display(summary_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
