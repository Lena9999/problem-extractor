{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_ai import OpenAIClient\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from parallel_post_processor import ParallelPostProcessor \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the previously pre-processed dataset\n",
    "file_path = os.getcwd() + \"/dataset/processed_dataset.csv\"\n",
    "data_set = pd.read_csv(file_path)\n",
    "data_set.rename(columns={\"ID\": \"id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's move on to data labeling using the GPT API.\n",
    "\n",
    "gpt-4o-mini:\n",
    "\n",
    "- Maximum 500 requests per minute.\n",
    "- Token limit is up to 200,000 per minute (higher than gpt-4o).\n",
    "- Maximum 10,000 requests per day.\n",
    "\n",
    "Due to the limitation of a maximum of 10,000 requests per day, we will split our dataset into two parts. First, we will label the first part of the dataset, and then the second part.\n",
    "\n",
    "#### Important conclusions\n",
    "\n",
    "- To avoid the issue of a limited number of daily requests and to reduce the total number of requests, it would be more efficient to structure the prompt in a way that allows for the analysis of two or more texts (since their combined length is less than 5,000 words) in a single request.\n",
    "\n",
    "- The class that implements parallel request sending (ParallelPostProcessor) can be improved. For example, recording the response result after each request is less efficient than implementing a batch write after N requests. However, for our case with a small dataset, this is not critical.\n",
    "\n",
    "- At the end, we need to merge 10 files with processed datasets. This could become an issue with larger datasets, so it may be worth improving this process for better efficiency.\n",
    "\n",
    "- In the case of a very large number of errors (which isn't our case, as our error rate is below one percent), error handling could cause delays due to the use of locks in the current implementation.\n",
    "\n",
    "- So far, we haven't been able to fully generalize the module (ParallelPostProcessor), as it remains tailored to the specific format of GPT responses used in our task. This is something that may need further refinement.\n",
    "\n",
    "\n",
    "##### The labeling time for half of the dataset (6507 posts) was 20 minutes. The cost is 41 cents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = len(data_set) // 2\n",
    "\n",
    "data_set_part1 = data_set.iloc[:split_index + 1]\n",
    "data_set_part2 = data_set.iloc[split_index + 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a request you need to have an API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "configs_name = \"configs/open_ai_access.json\"\n",
    "configs_name_path = os.path.join(current_directory, configs_name)\n",
    "with open(configs_name_path, 'r') as json_file:\n",
    "    open_ai_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to implement multiprocessing for processing posts, as this can take a considerable amount of time. To achieve this, we will use the ParallelPostProcessor class from the parallel_post_processor module. The results_dir is the directory where the processing results will be stored. We split our process into 10 parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_labeling_data = os.path.join(current_directory, \"dataset\")\n",
    "parallel_p_p = ParallelPostProcessor(results_dir=path_to_labeling_data, api_key=open_ai_data[\"api_key\"], organization=open_ai_data[\"organization\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is necessary to formulate the prompt correctly. Playground was used for testing and making the final selection of the prompt.\n",
    "\n",
    "In addition to the main task, we will also ask the chat to determine the gender and age of the author of the post, if possible. Maybe in the future this will be interesting for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Analyze the following text and answer three questions:\n",
    "\n",
    "Text: \"{text}\"\n",
    "1. What personal problems does the author describe about themselves? Only include issues that are explicitly stated as part of the author's own life **strictly in Russian**.\n",
    "2. What is the gender of the author? Answer with \"male\", \"female\". If gender cannot be determined from the text, leave the gender \"unspecified\". Pay close attention to grammatical cues such as verb forms, pronouns, and adjectives that indicate gender **strictly in English**.\n",
    "3. If the author explicitly states their age, extract it. If no age is mentioned, return \"unspecified\" **strictly in English**.\n",
    "Generate the response **in valid JSON format**.\n",
    "\n",
    "The response should look like this:\n",
    "\n",
    "{{\n",
    "  \"problems\": [\"[describe the problem 1]\", \"[describe the problem 2]\", ...],\n",
    "  \"gender\": \"[male/female/unspecified]\",\n",
    "  \"age\": \"[please specify age or \"unspecified\"]\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lenagogoleva/Desktop/nlprojects/problem-extractor/venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100/6506 posts processed.\n",
      "Progress: 200/6506 posts processed.\n",
      "Progress: 300/6506 posts processed.\n",
      "Progress: 400/6506 posts processed.\n",
      "Progress: 500/6506 posts processed.\n",
      "Progress: 600/6506 posts processed.\n",
      "Progress: 700/6506 posts processed.\n",
      "Progress: 800/6506 posts processed.\n",
      "Progress: 900/6506 posts processed.\n",
      "Progress: 1000/6506 posts processed.\n",
      "Progress: 1100/6506 posts processed.\n",
      "Progress: 1200/6506 posts processed.\n",
      "Progress: 1300/6506 posts processed.\n",
      "Progress: 1400/6506 posts processed.\n",
      "Progress: 1500/6506 posts processed.\n",
      "Progress: 1600/6506 posts processed.\n",
      "Progress: 1700/6506 posts processed.\n",
      "Progress: 1800/6506 posts processed.\n",
      "Progress: 1900/6506 posts processed.\n",
      "Progress: 2000/6506 posts processed.\n",
      "Progress: 2100/6506 posts processed.\n",
      "Progress: 2200/6506 posts processed.\n",
      "Progress: 2300/6506 posts processed.\n",
      "Progress: 2400/6506 posts processed.\n",
      "Progress: 2500/6506 posts processed.\n",
      "Progress: 2600/6506 posts processed.\n",
      "Progress: 2700/6506 posts processed.\n",
      "Progress: 2800/6506 posts processed.\n",
      "Progress: 2900/6506 posts processed.\n",
      "Progress: 3000/6506 posts processed.\n",
      "Progress: 3100/6506 posts processed.\n",
      "Progress: 3200/6506 posts processed.\n",
      "Progress: 3300/6506 posts processed.\n",
      "Progress: 3400/6506 posts processed.\n",
      "Progress: 3500/6506 posts processed.\n",
      "Progress: 3600/6506 posts processed.\n",
      "Progress: 3700/6506 posts processed.\n",
      "Progress: 3800/6506 posts processed.\n",
      "Progress: 3900/6506 posts processed.\n",
      "Progress: 4000/6506 posts processed.\n",
      "Progress: 4100/6506 posts processed.\n",
      "Progress: 4200/6506 posts processed.\n",
      "Progress: 4300/6506 posts processed.\n",
      "Progress: 4400/6506 posts processed.\n",
      "Progress: 4500/6506 posts processed.\n",
      "Progress: 4600/6506 posts processed.\n",
      "Progress: 4700/6506 posts processed.\n",
      "Progress: 4800/6506 posts processed.\n",
      "Progress: 4900/6506 posts processed.\n",
      "Progress: 5000/6506 posts processed.\n",
      "Progress: 5100/6506 posts processed.\n",
      "Progress: 5200/6506 posts processed.\n",
      "Progress: 5300/6506 posts processed.\n",
      "Progress: 5400/6506 posts processed.\n",
      "Progress: 5500/6506 posts processed.\n",
      "Progress: 5600/6506 posts processed.\n",
      "Progress: 5700/6506 posts processed.\n",
      "Progress: 5800/6506 posts processed.\n",
      "Progress: 5900/6506 posts processed.\n",
      "Progress: 6000/6506 posts processed.\n",
      "Progress: 6100/6506 posts processed.\n",
      "Progress: 6200/6506 posts processed.\n",
      "Progress: 6300/6506 posts processed.\n",
      "Progress: 6400/6506 posts processed.\n",
      "Progress: 6500/6506 posts processed.\n"
     ]
    }
   ],
   "source": [
    "parallel_p_p.process_dataset_in_parallel(dataset = data_set_part2,prompt_template=prompt, num_processes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to go through all files where the processes recorded labeled data results, combine them for both parts of the dataset, and then save the result to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    'folder_to_first_labeled_dataset',\n",
    "    'folder_to_second_labeled_dataset'\n",
    "]\n",
    "\n",
    "column_names = ['id', 'problems', 'gender', 'age']\n",
    "data_set_labeled = pd.DataFrame(columns=column_names)\n",
    "for folder_path in folders:\n",
    "    for i in range(10):\n",
    "        file_path = os.path.join(folder_path, f'successful_requests_process_{i}.txt')\n",
    "        \n",
    "      \n",
    "        current_process = pd.read_csv(file_path, delimiter=';', header=None)\n",
    "        current_process.columns = column_names\n",
    "        data_set_labeled = pd.concat([data_set_labeled, current_process], ignore_index=True)\n",
    "\n",
    "data_set_labeled.to_csv('folder_to_final_joined_dataset', index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
